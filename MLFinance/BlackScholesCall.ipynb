{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><center>Regression and Deep Learning in finance - notebook 1: European Call in Black & Scholes</center></h2>\n",
    "\n",
    "<center>Antoine Savine, October 2019</center>\n",
    "\n",
    "This notebook demonstrates how machines may be trained to learn derivatives pricing, in a simple context. We consider first a (polynomial) basis function regression, then a (vanilla) artificial neural network (ANN). We aim to learn the future price in 1y of a 2y call option of some strike $K$, as a function of the then prevailing underyling asset price $S_1$. Models learn from samples simulated in the Black & Scholes model.\n",
    "\n",
    "More precisely, the training set is generated with Monte-Carlo under Black & Scholes' dynamics with a given volatility $\\sigma$, and no rates or dividends. Each scenario consists in a sample $\\left(X, Y \\right)$, where $X = S_1$ is the underlying asset price in 1y, and $Y = max \\left( 0, S_2 - K \\right)$ samples the corresponding payoff of the call in 2y in the same scenario. \n",
    "\n",
    "Volatility is increased during the first period ($0$ to $T_1$). We widen the distribution of $X = S_1$ to increase data in the wings, without change to the relationship between $X = S_1$ and  $Y = max \\left( 0, S_2 - K \\right)$, which is what we aim to learn. This is a simple but effective means to mitigate extrapolation, a persisting problem in financial machine learning.\n",
    "\n",
    "The correct theoretical price of the option in 1y is given by Black & Scholes's formula: \n",
    "\n",
    "$${V_1} = E\\left[ {\\max \\left( 0, S_2 - K \\right)\\left| S_1 \\right.} \\right] = BS_{K,T_2-T_1,\\sigma }\\left( S_1 \\right)= S_1N\\left( {\\frac{{\\log \\left( {\\frac{S_1}{K}} \\right) + \\frac{{{\\sigma ^2\\left(T_2-T_1\\right)}}}{2}}}{\\sigma\\sqrt{T_2-T_1} }} \\right) - KN\\left( {\\frac{{\\log \\left( {\\frac{S_1}{K}} \\right) - \\frac{{{\\sigma ^2\\left(T_2-T-1\\right)}}}{2}}}{\\sigma\\sqrt{T_2-T_1} }} \\right)$$\n",
    "\n",
    "The prediction function given by machine learning models, correctly trained to minimize mean square error on data sampled from Black & Scholes' model, should converge towards this conditional expectation, as explained in the lectures.\n",
    "\n",
    "We assess performance by comparing prediction of trained models with Black & Scholes' formula. Importantly, machine learning models have no knowledge of Black & Scholes: they don't know how the data was generated, and they don't have any clue about the correct result. The correct prices are only calculated (with Black and Scholes formula) for our information. Machine learning models only see the simulated data and learn only from it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Simulation of the training set</center></h3>\n",
    "\n",
    "First, we simulate the training set under Black & Scholes dynamics: \n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "{S_1} = X = {S_0}\\exp \\left( {\\frac{{ - {\\sigma_0^2 T_1}}}{2} + {\\sigma}_0 \\sqrt{T1} {N_1}} \\right)\\\\\n",
    "{S_2} = {S_1}\\exp \\left( {\\frac{{ - {\\sigma ^2 \\left(T_2-T_1\\right)}}}{2} + \\sigma \\sqrt{T_2-T_1} {N_2}} \\right)\\\\\n",
    "Y = \\max \\left( {0,{S_2} - K} \\right)\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $N_1$ and $N_2$ are independent standard Gaussians."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# simulation with NumPy\n",
    "import numpy as np\n",
    "\n",
    "nSimul = 32768\n",
    "T1 = 1.0\n",
    "T2 = 2.0\n",
    "K = 110.0\n",
    "\n",
    "spot = 100.0\n",
    "vol = 0.2\n",
    "vol0 = 0.5 # vol is increased over the 1st period so we have more points in the wings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# simulate all Gaussian returns (N1, N2) first\n",
    "# returns: matrix of shape [nSimul, TimeSteps=2]\n",
    "returns = np.random.normal(size=[nSimul,2])\n",
    "\n",
    "# generate paths, step by step, and not path by path as customary\n",
    "# this is to avoid slow Python loops, using NumPy's optimized vector functions instead\n",
    "\n",
    "# generate the vector of all scenarios for S1, of shape [nSimul]\n",
    "S1 = spot * np.exp(-0.5*vol0*vol0*T1 + vol0*np.sqrt(T1)*returns[:,0])\n",
    "\n",
    "# generate the vector of all scenarios for S2, of shape [nSimul]\n",
    "S2 = S1 * np.exp(-0.5*vol*vol*(T2-T1) + vol*np.sqrt(T2-T1)*returns[:,1])\n",
    "\n",
    "# training set, X and Y are both vectors of shape [nSimul]\n",
    "X = S1\n",
    "Y = np.maximum(0, S2 - K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# display simulated data\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "import matplotlib.animation as animation\n",
    "mpl.rc('animation', html='jshtml')\n",
    "mpl.rcParams[\"animation.embed_limit\"] = 50\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "ax.set_xlim(0,250)\n",
    "ax.set_ylim(-50,200)\n",
    "\n",
    "ax.plot(X,Y, 'co', markersize=5, markerfacecolor=\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Linear and polynomial regressions</center></h3>\n",
    "\n",
    "Now we have a training set, we can train ML models. \n",
    "\n",
    "First, we train polynomial regression models. Recall, this is a linear regression over the monomials of $x$ up to some degree. Like for all linear models (where the prediction is a linear function of the weights), the solution is exact and unique, given by the well-known \"normal equation\". We could easily implement it ourselves, but this is a bit tedious (we must build all the monomials, etc.) and implemented in many standard libraries. We use the implementation of sklearn, wrapping it into a custom class for convenience.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# polynomial regression in sklearn\n",
    "from sklearn.preprocessing import PolynomialFeatures  \n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# simple wrapper class for multi-dimensional polynomial regression\n",
    "class PolyReg:\n",
    "    \n",
    "    def __init__(self, X, Y, degree):\n",
    "        \n",
    "        # create monomials\n",
    "        self.features = PolynomialFeatures(degree = degree)  \n",
    "        self.monomials = self.features.fit_transform(X)\n",
    "        \n",
    "        # regress with normal equation\n",
    "        self.model = LinearRegression()  \n",
    "        self.model.fit(self.monomials, Y)\n",
    "\n",
    "    def predict(self, x):\n",
    "        \n",
    "        # predict with dot product\n",
    "        monomials = self.features.fit_transform(x)\n",
    "        return self.model.predict(monomials)\n",
    "\n",
    "# run regressions of degree = 1 to 6\n",
    "polyRegs = [PolyReg(X[:, np.newaxis], Y, degree) for degree in range(1,7)]\n",
    "# the (perhaps curious) syntax X[:, np.newaxis] means \n",
    "# turn the vector X of shape [nSimul] into a matrix of shape [nSimul, 1]\n",
    "# which is the same thing, but of the shape expected by sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# compute and display results\n",
    "\n",
    "# compute\n",
    "xAxis = np.linspace(20, 200, 100)\n",
    "poly = [polyReg.predict(xAxis[:, np.newaxis]) for polyReg in polyRegs]\n",
    "\n",
    "# display\n",
    "fig, ax = plt.subplots(2, 3)\n",
    "fig.set_size_inches(15, 10)\n",
    "for i in [0,1]:\n",
    "    for j in [0,1,2]:\n",
    "        polIdx = 3*i + j\n",
    "        ax[i, j].set_xlim(0,250)\n",
    "        ax[i, j].set_ylim(-25,200)\n",
    "        ax[i,j].set_title(\"degree: \" + str(1 + polIdx))\n",
    "        ax[i,j].plot(X,Y, 'c.', markersize=1)\n",
    "        ax[i,j].plot(xAxis, poly[polIdx], 'b-')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Black & Scholes' formula</center></h3>\n",
    "\n",
    "To assess accuracy, we implement Black & Scholes's formula, for our information only, to compare model predicitions with the correct results:\n",
    "\n",
    "$$\n",
    "B{S_{K,T,\\sigma }}\\left( S \\right) = SN\\left( {\\frac{{\\log \\left( {\\frac{S}{K}} \\right) + \\frac{{{\\sigma ^2 T}}}{2}}}{\\sigma \\sqrt{T} }} \\right) - KN\\left( {\\frac{{\\log \\left( {\\frac{S}{K}} \\right) - \\frac{{{\\sigma ^2 T}}}{2}}}{\\sigma\\sqrt{T} }} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import norm # cumulative normal distribution\n",
    "\n",
    "# general formula\n",
    "def BlackScholes(spot, strike, vol, T):\n",
    "    d1 = (np.log(spot/strike) + vol * vol * T) / vol / np.sqrt(T)\n",
    "    d2 = d1 - vol * np.sqrt(T)\n",
    "    return spot * norm.cdf(d1) - strike * norm.cdf(d2)\n",
    "\n",
    "# in our example, with fixed strike, vol and expiry\n",
    "def bs(spot):\n",
    "    return BlackScholes(spot, K, vol, T2 - T1)\n",
    "\n",
    "# test\n",
    "print(\"%.4f\" % BlackScholes(100, 100, .1, 1.)) # 3.98"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now visualize the quality of the polynomial regressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tgt = bs(xAxis)\n",
    "\n",
    "# display\n",
    "fig, ax = plt.subplots(2, 3)\n",
    "fig.set_size_inches(15, 10)\n",
    "for i in [0,1]:\n",
    "    for j in [0,1,2]:\n",
    "        polIdx = 3*i + j\n",
    "        ax[i, j].set_xlim(0,250)\n",
    "        ax[i, j].set_ylim(-25,200)\n",
    "        ax[i,j].set_title(\"degree: \" + str(1 + polIdx))\n",
    "        ax[i,j].plot(X,Y, 'c.', markersize=1)\n",
    "        ax[i,j].plot(xAxis, poly[polIdx], 'b-')\n",
    "        ax[i,j].plot(xAxis, tgt, 'r-')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a custom visualizer, freely set degree and limits  \n",
    "\n",
    "# set here\n",
    "degree = 8\n",
    "xlim = (20, 300)\n",
    "ymax = 200\n",
    "#\n",
    "\n",
    "# display\n",
    "xs = np.linspace(xlim[0], xlim[1], 100)\n",
    "ys = bs(xs)\n",
    "\n",
    "polyReg = PolyReg(X[:, np.newaxis], Y, degree) \n",
    "polys = polyReg.predict(xs[:, np.newaxis]) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "ax.set_xlim(xlim[0], xlim[1])\n",
    "ax.set_ylim(-25,ymax)\n",
    "ax.plot(X,Y, 'c.', markersize=1)\n",
    "ax.plot(xs, polys, 'b-')\n",
    "ax.plot(xs, ys, 'r-')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(With perhaps the exception of extrapolation) the performance is remarkable: the regression models don't know anything about Black & Scholes. All they see are the samples, a could of points that would look random to even experienced humans. Yet, the machine finds patterns in the data and identifies a function that approaches Black & Scholes to remarkable accuracy over a wide range of about 50-200 for such a simple method.\n",
    "\n",
    "Note accuracy is not sufficient for pricing or risk management (although such accuracy can be achieved, with a much larger training set) but it enough for instance, for capital and risk simulations of large, complex trading books like Counterparty Value Adjustment (CVA)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><center>Deep learning</center></h3>\n",
    "\n",
    "We now implement a vanilla neural network to perform the regression in place of conventional linear or polynomial (or any other type of basis) regressions. Neural networks don't regress on a fixed set of basis functions, they learn the best functions (also known as features, or best representation in ML lingo), from the data, in their hidden layers, by composing activated combinations of previous layers. This is what allows neural networks to scale to high dimensional inputs, regressing on a low number of \"best\" representations, while the number of monomials (or any other kind of fixed basis functions) would increase exponentially in a conventional regression. With low dimensional problems like our simple example, we should expect neural nets to perform at least as well as conventional regression.\n",
    "\n",
    "We implement a simple diamond-shaped network of the form:\n",
    "\n",
    "<img src=\"net1.png\">\n",
    "\n",
    "where the input layer has a single input $S_1$, and we have a number of hidden layers so the net can learn to compose features for the regression on the final layer. We compute each layer from the previous layer with the classic feed-forward equation:\n",
    "\n",
    "$$\n",
    "{a^{\\left[ l \\right]}} = {elu}\\left( {{W^{\\left[ l \\right]}}{a^{\\left[ {l - 1} \\right]}} + {b^{\\left[ l \\right]}}} \\right)\n",
    "$$\n",
    "\n",
    "The output is computed by linear regression onto the basis functions in the final hidden layer. \n",
    "\n",
    "We activate the hidden layers with the ELU function $elu\\left( x \\right) = x{1_{\\left\\{ {x > 0} \\right\\}}} + \\left( {{e^x} - 1} \\right){1_{\\left\\{ {x \\le 0} \\right\\}}}$. Another natural choice is the similar softplus function $softPlus\\left( x \\right) = \\log \\left( {1 + {e^x}} \\right)$. ELU has been (empirically) shown to improve training over the classic $relu\\left( x \\right) = {x^ + }$. ELU has linear asymptotes, which is the desired behaviour in valuation problems: financial products are often linearly extrapolated and this behaviour is often enforced. For example, in finite difference methods, we generally work with linear boundary conditions. ELU is also considered a best practice presently in the deep learning community, for very different reasons related to speed and stability of numerical optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# training works best in normalized space, so we normalized our inputs and labels \n",
    "\n",
    "meanX = np.mean(X)\n",
    "stdX = np.std(X)\n",
    "meanY = np.mean(Y)\n",
    "stdY = np.std(Y)\n",
    "\n",
    "normX = (X - meanX) / stdX\n",
    "normY = (Y - meanY) / stdY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# start TensorFlow\n",
    "\n",
    "# Google colab magic for choosing tf version \n",
    "# makes workbooks fine for uploading to colab -- uncomment below when on colab\n",
    "# %tensorflow_version 1.x\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "print(tf.test.is_gpu_available())\n",
    "\n",
    "# disable annoying warnings\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "\n",
    "# clear calculation graph\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# inference = sequence of feed-forward equations from input to output \n",
    "# TensorFlow provides higher level function for all kinds of standard layers\n",
    "# for vanilla layers, the function is tf.layers.dense() \n",
    "\n",
    "# the weights and biases are encapsulated and do not explicitly appear in the code\n",
    "\n",
    "# the argument kernel_initializer allows to control the initialization of the weights\n",
    "# (the biases are all initialized to 0)\n",
    "# tf.variance_scaling_initializer() implements the Xavier-He initialization\n",
    "# (centred Gaussian with variance 1.0 / num_inputs)\n",
    "# widely considered an effective default, see e.g. Andrew Ng's DL spec on Coursera\n",
    "\n",
    "def inference(xs):\n",
    "    \n",
    "    # hidden layers, note that the weights and biases are encpasulated in the tf functions\n",
    "    a1 = tf.layers.dense(xs, 3, activation = tf.nn.elu, kernel_initializer = tf.variance_scaling_initializer)\n",
    "    a2 = tf.layers.dense(a1, 5, activation = tf.nn.elu, kernel_initializer = tf.variance_scaling_initializer)\n",
    "    a3 = tf.layers.dense(a2, 3, activation = tf.nn.elu, kernel_initializer = tf.variance_scaling_initializer)\n",
    "    \n",
    "    # output payer\n",
    "    ys = tf.layers.dense(a3, 1, activation = None, kernel_initializer = tf.variance_scaling_initializer)\n",
    "    \n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# calculation graph for prediction and loss\n",
    "\n",
    "# the instructions below don't calculate anything, they initialize a calculation graph in TensorFlow's memory space\n",
    "# when the graph is complete, we can run it in a TensorFlow session, on CPU or GPU\n",
    "\n",
    "# since TensorFlow knows the calculation graph, it will not only evaluate the results, but also the gradients, \n",
    "# very effectively, with the back-propagation equations\n",
    "\n",
    "# reserve space for inputs and labels\n",
    "inputs = tf.placeholder(shape=[None,1], dtype = tf.float32)\n",
    "labels = tf.placeholder(shape=[None,1], dtype = tf.float32)\n",
    "\n",
    "# calculation graphs for predictions given inputs and loss (= mean square error) given labels\n",
    "predictions = inference(inputs)\n",
    "loss = tf.losses.mean_squared_error(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# definition of the optimizer\n",
    "# the optimizer computes the gradient of loss to all weights and biases,\n",
    "# and modifies them all by a small step (learning rate) in the direction opposite to the gradient\n",
    "# in order to progressively decrease the loss and identify the set of weights that minimize it\n",
    "\n",
    "learning_rate = 0.05\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate) # optimizer obejct\n",
    "optimize = optimizer.minimize(loss) #  this op now computes gradient and moves weights\n",
    "# the op 'optimize' performs one iteration of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can display predictions before, during and after training\n",
    "# to do this, we execute the inference result named 'predictions' on the session\n",
    "# with some arbitrary inputs xs\n",
    "def predict(xs):\n",
    "    # first, normalize\n",
    "    nxs = (xs - meanX) / stdX\n",
    "    # forward feed through ANN\n",
    "    nys = sess.run(predictions,feed_dict={inputs:np.reshape(nxs, [-1,1])})\n",
    "    # de-normalize output\n",
    "    ys = meanY + stdY * nys\n",
    "    # we get a matrix of shape [size of xs][1], which we reshape as vector [size of xs]\n",
    "    return np.reshape(ys, [-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we can also display the resulting function \n",
    "# run the ann on the xaxis and compare to Black & Scholes \n",
    "def display(epoch):\n",
    "    fig, ax = plt.subplots()\n",
    "    fig.set_size_inches(10, 8)\n",
    "    ax.set_xlim(0,250)\n",
    "    ax.set_ylim(-25,200)\n",
    "    plt.title(\"epoch: \" + str(epoch))\n",
    "    plt.plot(X,Y, 'c.', markersize=1)\n",
    "    # call to predict() with inputs = xAxis\n",
    "    # recall xAxis = grid over wide range of spots\n",
    "    plt.plot(xAxis, predict(xAxis), 'b-')\n",
    "    # recall tgt = black scholes applied to xAxis\n",
    "    plt.plot(xAxis, tgt, 'r-')\n",
    "    #\n",
    "    plt.show()\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# now the graph is in memory, TensorFlow's Session object can execute it on a session\n",
    "\n",
    "# start the session\n",
    "sess = tf.Session()\n",
    "\n",
    "# initialize all variables (weights + biases on all layers)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we want to compute the predictions and losses on the session\n",
    "# remember the graph has some placeholders for inputs and labels\n",
    "# we must feed actual values for those into the session\n",
    "\n",
    "# this is performed with a Python dictionary of placeholder names to concrete values\n",
    "\n",
    "# we train the network to best predict the ~normalized~ ys out of the ~normalized~ xs\n",
    "feed_dict = {inputs:normX[:,np.newaxis], labels:normY[:,np.newaxis]}\n",
    "# TensorFlow expects matrices (possibly with 1 column, not vectors, so we must reshape our vector data with np.newaxis)\n",
    "\n",
    "# visualize the initial (random) function\n",
    "display(0)\n",
    "\n",
    "# compute and display the initial loss\n",
    "print (\"initial loss = %.4f\" % sess.run(loss, feed_dict=feed_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# run the optimizer once\n",
    "# note that TensorFlow silently abd efficiently computes the gradient by back-propagation along the calculation graph\n",
    "sess.run(optimize, feed_dict=feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# recalculate loss, it is now lower due to moving the weights\n",
    "print (\"improved loss = %.4f\" % sess.run(loss, feed_dict=feed_dict))\n",
    "\n",
    "# and see the (slightly) improved function representation\n",
    "display(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss decreased, and the learned frunction is a bit closer to correct\n",
    "# let us apply 50 more iterations (or \"epochs\")\n",
    "\n",
    "for _ in range(50):\n",
    "    sess.run(optimize, feed_dict=feed_dict)\n",
    "\n",
    "print (\"improved loss = %.4f\" % sess.run(loss, feed_dict=feed_dict))\n",
    "display(51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# good, now we learn for real: re-initialize and run 1,000 epochs\n",
    "epochs = 1000\n",
    "\n",
    "# we save results after each epoch for visualization\n",
    "yAxes = [] \n",
    "\n",
    "# re-initialize\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# go\n",
    "for _ in range(epochs):\n",
    "    yAxes.append(predict(xAxis))\n",
    "    sess.run(optimize, feed_dict=feed_dict)\n",
    "\n",
    "# and see improved results\n",
    "print (\"final loss = %.4f\" % sess.run(loss, feed_dict=feed_dict))\n",
    "display(epochs)\n",
    "\n",
    "# note computation time is not representative, most of the time is spent saving results for visualization\n",
    "# training alone completes in a fraction of a second"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "We applied a few iterations of the naive gradient descent (GD) algorithm, which is nice for pedagogy because it is easy to understand. It is also effective enough for our simple problem. But this is not current best practice in Deep Learning.\n",
    "\n",
    "Best practice is train by mini-batch, that is, compute gradients and update weights, not with the entire training set, but a small subset of it (say, 256 samples). This way, updates are applied faster while the estimated gradient that is applied is unbiased (correct in expectation). This is called stochastic gradient descent (SGD) and converges under the same conditions as GD.\n",
    "\n",
    "Further, best practice is replace standard gradient descent with heuristic improvements, like ADAM (see Wikipedia [en.wikipedia.org/wiki/Stochastic_gradient_descent] (https://en.wikipedia.org/wiki/Stochastic_gradient_descent) or Deep Learning spec on Coursera). These improvements compute gradients in the same way but apply them differently, with  notions like momentum (apply weighted average of recent gradients rather than current) or adaptative leraning rates (typically, normalized by recent standard deviation of gradients in different dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# create animation to better visualize how the model learns\n",
    "\n",
    "# create figure\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches(10, 8)\n",
    "ax.set_xlim(0,250)\n",
    "ax.set_ylim(-25,200)\n",
    "ax.plot(X,Y, 'c.', markersize=1)\n",
    "ax.plot(xAxis, tgt, 'r-')\n",
    "ax.set_title(\"european call in black & scholes - epoch = 0\")\n",
    "line, = ax.plot(xAxis, yAxes[0], 'b-')\n",
    "\n",
    "def anim_init():\n",
    "    ax.set_title(\"european call in black & scholes - epoch = 0\")\n",
    "    line.set_ydata(yAxes[0])\n",
    "    return line, \n",
    "\n",
    "def anim_update(i):\n",
    "    if i > -1:\n",
    "        ax.set_title(\"european call in black & scholes - epoch = \" + str(i+1))\n",
    "        line.set_ydata(yAxes[i])\n",
    "    return line, \n",
    "\n",
    "anim = animation.FuncAnimation(fig, anim_update, init_func = anim_init, \n",
    "                               frames=np.arange(-1, 1000, 5), interval = 40, blit=True, repeat=False)\n",
    "plt.close(fig)\n",
    "\n",
    "anim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance is generally similar to polynomial regression. The main benefit of deep learning is that it scales to high dimensional inputs, while the number of basis functions in conventional regression grows exponentially with the dimension of the input vector. In this example, dimension is 1 so neural nets don't make much difference, although it is interesting to visualize how they learn, and this also allowed us to implement our first ANN in a particularly simple context.\n",
    "\n",
    "Next, we repeat the experiement with higher dimensional basket options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
